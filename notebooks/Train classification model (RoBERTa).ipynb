{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataiku/code-envs/python/TONE_py3_env/lib/python3.6/site-packages/packaging/version.py:114: DeprecationWarning: Creating a LegacyVersion has been deprecated and will be removed in the next major release\n",
      "  DeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "import dataiku\n",
    "import dataikuapi\n",
    "from dataiku import pandasutils as pdu\n",
    "from dataiku import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import RobertaForMaskedLM, RobertaModel\n",
    "from transformers import TFRobertaForSequenceClassification, TFRobertaModel\n",
    "from transformers import pipeline\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_input(sentences,tokenizer):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "  \n",
    "    for x in sentences:\n",
    "        inputs = tokenizer.encode_plus(x, \n",
    "                                          add_special_tokens=True, \n",
    "                                          return_token_type_ids=True,\n",
    "                                          truncation=True, \n",
    "                                          max_length=max_length)\n",
    "        i, t = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "        m = [1] * len(i)\n",
    "\n",
    "        padding_length = max_length - len(i)\n",
    "\n",
    "        i = i + ([pad_token] * padding_length)\n",
    "        m = m + ([0] * padding_length)\n",
    "        t = t + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        input_ids.append(i)\n",
    "        attention_masks.append(m)\n",
    "        token_type_ids.append(t)\n",
    "  \n",
    "    return [np.asarray(input_ids), \n",
    "            np.asarray(attention_masks), \n",
    "            np.asarray(token_type_ids)]\n",
    "\n",
    "def example_to_features(input_ids, attention_masks, token_type_ids, y):\n",
    "    return {\"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"token_type_ids\": token_type_ids},y\n",
    "\n",
    "def predict(tokenizer, model, sentences):\n",
    "    tf_batch = tokenizer(sentences, max_length=256, padding=True, truncation=True, return_tensors='tf')\n",
    "    tf_outputs = model(tf_batch)\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "    label = tf.argmax(tf_predictions, axis=1)\n",
    "    label = label.numpy()\n",
    "    return label\n",
    "\n",
    "def predict_alt(tokenizer, model, sentences):\n",
    "    tf_batch = tokenizer(sentences, max_length=256, padding=True, truncation=True, return_tensors='tf')\n",
    "    tf_outputs = model(tf_batch)\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "    label = tf.argmax(tf_predictions, axis=1)\n",
    "    label = label.numpy()\n",
    "    return label, tf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NL = '/data/library/python/robbert-v2-dutch-base'\n",
    "MODEL_EN = '/data/library/python/roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at /data/library/python/robbert-v2-dutch-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_nl = TFRobertaForSequenceClassification.from_pretrained(MODEL_NL,num_labels=2)\n",
    "tokenizer_nl = RobertaTokenizer.from_pretrained(MODEL_NL)\n",
    "\n",
    "# model_en = TFRobertaForSequenceClassification.from_pretrained(MODEL_EN,num_labels=4)\n",
    "# tokenizer_en = RobertaTokenizer.from_pretrained(MODEL_EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset('df_results_mvb_big_labelled').get_dataframe().set_index('col_0')\n",
    "df_nl = df[df['dc:language']=='nl'].reset_index()\n",
    "df_en = df[df['dc:language']=='en'].reset_index()\n",
    "dft = df_nl[df_nl.label.isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataiku/code-envs/python/TONE_py3_env/lib64/python3.6/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# necessary step because otherwise the tokenizer produces incorrect token during training\n",
    "for row in dft.index:\n",
    "    dft.loc[row, 'dnb_nlp:formatted'] = dft.loc[row, 'dnb_nlp:formatted'].replace(\"\\n\", \" \")\n",
    "# for row in df_en.index:\n",
    "#     df_en.loc[row, 'dnb_nlp:sentence'] = df_en.loc[row, 'dnb_nlp:sentence'].replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dutch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = (np.array(dft['dnb_nlp:formatted']))\n",
    "# y = (np.array(dft['label']))\n",
    "# X = np.array([test.encode(\"ascii\", \"ignore\").decode() for test in list(X)])\n",
    "# X = np.array([test.replace('\\n',' ') for test in list(X)])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# print(\"Train dataset shape: {0}, \\nTest dataset shape: {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# pad_token = 0\n",
    "# pad_token_segment_id = 0\n",
    "# max_length = 256\n",
    "\n",
    "# X_test_input = convert_to_input(X_test,tokenizer_nl)\n",
    "# X_train_input = convert_to_input(X_train,tokenizer_nl)\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((X_train_input[0],X_train_input[1],X_train_input[2],y_train)).map(example_to_features).shuffle(100).batch(32).repeat(5)\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((X_test_input[0],X_test_input[1],X_test_input[2],y_test)).map(example_to_features).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, \n",
    "#                                      epsilon=1e-08, \n",
    "#                                      clipnorm=1.0)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #can be used when there are two or more label classes\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "# model_nl.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "# model_nl.fit(train_ds, epochs=5, validation_data=test_ds) #train model for finxed number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Lc1UMaK3 is code of model_nl_methods\n",
    "# model_dir = '/data/dataiku/managed_folders/solvency2/TV_TEXTMINING/Lc1UMaK3'\n",
    "# model_nl.save_pretrained(save_directory=model_dir)\n",
    "\n",
    "# # folder = dataiku.Folder('c9VWnxpk')  \n",
    "# # model_nl.save_pretrained(save_directory=folder.get_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test performance Dutch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at /data/dataiku/managed_folders/solvency2/TV_TEXTMINING/Lc1UMaK3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = dict()\n",
    "tokenizer = dict()\n",
    "\n",
    "model_dir = '/data/dataiku/managed_folders/solvency2/TV_TEXTMINING/Lc1UMaK3'\n",
    "model['nl'] = TFRobertaForSequenceClassification.from_pretrained(model_dir,num_labels=2)\n",
    "tokenizer['nl'] = RobertaTokenizer.from_pretrained(MODEL_NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataiku/code-envs/python/TONE_py3_env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----++-----------+-----+----+-------------+-+++--+++++++++++++--++++++++-++++++++----------+--+++++----++---+----++--+-+-+----+-+-----++----------++---+---+++---+++++-++++++++++++-+--+++-+++-+++--------------------------------------------------------------------------------------------------------------+-+++++++++-++++-+++++++++++++-++++-+-+++---+++++++++++++++-+--+++++++++-++++++-+++++----++----------+----+-++++++-+++-++-++++-+-+--++++-++++-+-++++++-++++-++----++++--------------+++++++-++-++-----++++++++----+--------+-+---+---+++++++-+++-++++++++++---------+---+-------+++++---++++-------------------------+++++-++++-------------+-+-+++++++++----------------+-+-------+--+---+----+-+---+--+++----++-----++++-------++-----+--+++------+--+++++--------------------+-++++-+-+++++---+----------------++------------++---+--+++---++++-------------+++++++-----------------+++++++-++-++++++--++-+++--++++-+--+---------+----++++-+-+-++++++---+----++-----+-+-++++++++-++--+----++--+++--------+++--------+---------+-+-----+--------+--+-------++-------+------------+----------------+--++---+++--------+--+---+--++--+-++---------------------------------------------------------------------------------------------------------------------------------------+++-+++++-+-+-+++++++-++++++++++++++++++-++----+++-++++++++++++++++++++++++++-+++++++++-+-+-++--+-++-++++-+++++-+-++-+--++++++-+-++++++++--+++++-+++--++-+-+++-+++-+++++++--++---++--++++++-++-+++-++++-+++++-+++---+-+++++-+-++-+++-+++++++++-+++++++++++-+++---+-------+++--+--+++++++++++-+++++++++++-+++++++++++++-+++++---+--+-++-++-++++---+----+-----+++-+-++------------------+--++++-+++++++++---+-++++++++-++---++++-++-+++++++++-+++-++-++-++++-++++++++++---+--+++++++++++++--+-++-++-++-+-++++--++---++++-+---+-+++--++-+---++++++-++-+++++++-++++-+-+++-+-++-+++++++-+++--++++----+++++-++--+-++++++--+++++-+++++++--+--+--+-+---+-+-+++-++++-+----++---++-+-----++++++++-+-+++++++++++---+-++++++++++++------------------+++++-+++++++++---------------------++++-+------++++-+++--+----++-+-+-+-++++-+++-+-++----++++-+-----++-++++-+-+++++-+--++----[[1094   13]\n",
      " [  25  954]]\n",
      "0.9817833173537871\n",
      "0.9818345027963835\n",
      "0.9817833173537871\n",
      "0.9817762523796181\n"
     ]
    }
   ],
   "source": [
    "#Whole labelled set\n",
    "dft['prediction'] = -1  # sentence is not processed\n",
    "\n",
    "for row in dft.index:\n",
    "    p = predict(tokenizer['nl'],model['nl'], [dft.loc[row, 'dnb_nlp:formatted']])\n",
    "    print(\"+\" if p==1 else \"-\", end='')\n",
    "    dft.loc[row, 'prediction'] = p\n",
    "\n",
    "#Sentences wrong prediction\n",
    "#print(dft[(dft['label']!=dft['prediction'])]['dnb_nlp:sentence'].values)\n",
    "\n",
    "print(confusion_matrix(dft['label'], dft['prediction']))\n",
    "print(accuracy_score(dft['label'], dft['prediction']))\n",
    "print(precision_score(dft['label'], dft['prediction'],average='weighted'))\n",
    "print(recall_score(dft['label'], dft['prediction'],average='weighted'))\n",
    "print(f1_score(dft['label'], dft['prediction'],average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.6965838 0.3034161]], shape=(1, 2), dtype=float32)\n",
      "Bij wijze van  stresstest  heeft PME daarom ook een jaar ( 2035 ) met extreem weer laten doorrekenen .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[0.37110218 0.62889785]], shape=(1, 2), dtype=float32)\n",
      "VISIE PMT De inzet van PMT is het bieden van een betaalbaar en duurzaam pensioen voor alle deelnemers met maatwerk binnen de collectieve regeling .\n",
      "[[874   1]\n",
      " [  1 792]]\n",
      "0.9988009592326139\n",
      "0.9988009592326139\n",
      "0.9988009592326139\n",
      "0.9988009592326139\n"
     ]
    }
   ],
   "source": [
    "#Training set\n",
    "X = (np.array(dft['dnb_nlp:formatted']))\n",
    "y = (np.array(dft['label']))\n",
    "X = np.array([test.encode(\"ascii\", \"ignore\").decode() for test in list(X)])\n",
    "X = np.array([test.replace('\\n',' ') for test in list(X)])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "y_predict = []\n",
    "for i in range(0,len(X_train)):\n",
    "    p,prob = predict_alt(tokenizer['nl'],model['nl'], [X_train[i]])\n",
    "    #print(\"+\" if p==1 else \"-\", end='')\n",
    "    if p != y_train[i]:\n",
    "        print(p)\n",
    "        print(y_train[i])\n",
    "        print(prob)\n",
    "        print(X_train[i])\n",
    "    y_predict = y_predict + list(p)\n",
    "\n",
    "print(confusion_matrix(y_train,y_predict))\n",
    "print(accuracy_score(y_train, y_predict))\n",
    "print(precision_score(y_train, y_predict,average='weighted'))\n",
    "print(recall_score(y_train, y_predict,average='weighted'))\n",
    "print(f1_score(y_train, y_predict,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[0.00126092 0.99873906]], shape=(1, 2), dtype=float32)\n",
      "van klimaatbestendige infrastructuur .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[0.00847464 0.99152535]], shape=(1, 2), dtype=float32)\n",
      "PME in 2018 geen investeringen binnen dit thema kunnen doen .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[0.2655244 0.7344757]], shape=(1, 2), dtype=float32)\n",
      "Een goed pensioen is er ook voor je partner Impact Investing is Samen met de nieuwe adviseur en de nieuwe uitvoerder van het vermogensbeheer het doen van gerichte willen we verder stappen zetten op dit gebied waarbij we ons in 2021 en 2022 zullen beleggingen welke richten op :\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[9.998259e-01 1.741027e-04]], shape=(1, 2), dtype=float32)\n",
      "5.2.5 Door transities betere aansluiting met ons maatschappelijk verantwoord beleggingsbeleid\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.99065405 0.00934594]], shape=(1, 2), dtype=float32)\n",
      "Uitsluiting wordt onder andere toegepast lijkt het erop dat we de komende jaren , vooral in de VS , ten aanzien van controversile wapens , tabak , thermische met hogere inflatie te maken krijgen .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[9.9974424e-01 2.5578614e-04]], shape=(1, 2), dtype=float32)\n",
      "De gedachte daarachter is dat bedrijven die er beter in slagen om conflic- terende eisen tegen elkaar af te wegen , zich beter kunnen aanpassen aan veranderende omstandigheden .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.5052769  0.49472308]], shape=(1, 2), dtype=float32)\n",
      " Extra aandacht voor duurzaamheidsrisicos Daarnaast vraagt DNB extra aandacht voor beheersing van duurzaamheidsrisicos door financile instellingen .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[0.0324911 0.9675089]], shape=(1, 2), dtype=float32)\n",
      "CO2-voetafdruk van de beleggingen\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.99733716 0.00266278]], shape=(1, 2), dtype=float32)\n",
      "Hiermee START IMPLEMENTATIE BELEID DUURZAAM EN VERANTWOORD BELEGGEN\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[4.1413074e-04 9.9958593e-01]], shape=(1, 2), dtype=float32)\n",
      "Duurzame consumentengoederen Niet-duurzame consumentengoederen Financile instellingen Gezondheidszorg Industrie Informatie technologie Bouw- en grondstoffen Telecommunicatie Energie Nutsbedrijven Overige Specificatie aandelen naar regio :\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[0.35811594 0.641884  ]], shape=(1, 2), dtype=float32)\n",
      "Het VO wordt graag in de gelegenheid gesteld een bijdrage te leveren aan de discussie en de vaststelling van het beleid in dezen .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.99793756 0.00206249]], shape=(1, 2), dtype=float32)\n",
      "met controversile wapens , zoals zich gedragen volgens de United Nations\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.90737087 0.09262917]], shape=(1, 2), dtype=float32)\n",
      "Infrastructuur wordt in de toekomst alleen overwogen wanneer er sprake is van de mogelijkheid van impact investing .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[9.9990296e-01 9.6993979e-05]], shape=(1, 2), dtype=float32)\n",
      "Een deel van de meer gedetailleerde regelgeving , de Regulatory Technical Standards ( RTS ) , was voorzien per 1 januari 2022 , maar is uitgesteld en wordt mogelijk pas per 1 januari 2023 van kracht ( level 2 ) .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.99279225 0.0072078 ]], shape=(1, 2), dtype=float32)\n",
      "164/176 CO2eq reductie aandelenportefeuille Waarde aandelen ( x  1 miljoen ) * CO2eq-uitstoot ( in ton ) CO2eq-uitstoot per  100 vermogen Vermindering CO2eq-uitstoot in % t.o.v. vorige meting Vermindering CO2eq-uitstoot in % t.o.v. 2015 * waarde aandelen ultimo voorgaand jaar CO2eq reductie aandelenportefeuille Genormaliseerde CO2eq-uitstoot per  100 vermogen Vermindering genormaliseerde CO2eq - uitstoot in % t.o.v. vorige meting Vermindering genormaliseerde CO2eq - uitstoot in % t.o.v. 2015 2020 19.074 2.086.126 10,9 - 29,4 % 2019 14.622 2.265.609 15,5 - 8,3 % 2018 16.646 2.810.504 16,9 - 18,6 % 2017 17.057 3.537.457 20,7 - 2,3 % 2016 14.037 2015 12.601 2.979.346 2.953.557 23,4 - 21,2 - 9,4 % - 53,3 % - 33,9 % - 28,0 % - 11,5 % -9,4 % - 2020 2019 2018 2017 2016 2015 17,8 20,6 23,4 24,4 22,2 23,4 - 13,6 % - 12,0 % - 4,1 % 9,9 % - 5,1 % - 23,9 % - 12,0 % 0,0 % 4,3 % - 5,1 % - - Overige doelstellingen en resultaten PME stelt jaarlijks  250 miljoen beschikbaar voor het doen van nieuwe impactinvesteringen .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[9.9980348e-01 1.9652396e-04]], shape=(1, 2), dtype=float32)\n",
      "Voor meer informatie over Publiceren verklaring informatie geldende wet- en regelgeving .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[0.13513045 0.8648695 ]], shape=(1, 2), dtype=float32)\n",
      "Het bestuur heeft dit opgenomen in de verklaring beleggingsbeginselen die op de website gepubliceerd zijn .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[1.4820417e-04 9.9985182e-01]], shape=(1, 2), dtype=float32)\n",
      "Het VO beseft dat we als relatief klein fonds minder middelen hebben dan de grote pensioenfondsen om te besteden aan dit thema .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.99791676 0.00208327]], shape=(1, 2), dtype=float32)\n",
      "Het Fonds realiseert zich daarbij dat het als lange termijn investeerder een maatschappelijke plicht heeft , maar ook dat directe invloed als lange termijn investeerder beperkt is .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.75502837 0.24497165]], shape=(1, 2), dtype=float32)\n",
      "Het gaat dan bijvoorbeeld om landen die sancties zijn opgelegd door de EU , de VN of de VS .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[9.9954766e-01 4.5236116e-04]], shape=(1, 2), dtype=float32)\n",
      "Uiteindelijk werden centrale banken richting inclusief de langetermijnwaardecreatie , en de themas het eind van het jaar gedwongen om dit te erkennen en van het pensioenfonds centraal .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.99124485 0.00875512]], shape=(1, 2), dtype=float32)\n",
      "Staatsobligaties van landen waarop een sanctie van de Verenigde Naties of de Europese Unie rust , zijn ook uitgesloten .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[0.10396176 0.89603823]], shape=(1, 2), dtype=float32)\n",
      "Op grond van de prudent person-regel sluit het pensioenfonds daarbij op voorhand geen enkele afzonderlijke beleggingscategorie uit .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[1.5490183e-04 9.9984503e-01]], shape=(1, 2), dtype=float32)\n",
      "het IMVB-convenant ) .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[3.956993e-04 9.996043e-01]], shape=(1, 2), dtype=float32)\n",
      "In 2021 hebben we die ambitie verder aangescherpt .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.998047   0.00195297]], shape=(1, 2), dtype=float32)\n",
      "De tota- le emissies voor een portefeuille worden genormaliseerd op basis van de marktwaarde van de portefeuille .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[9.9973291e-01 2.6704735e-04]], shape=(1, 2), dtype=float32)\n",
      " De verwachte verandering in chronische en acute ( weers-)omstandigheden op de verschillende locaties wordt ingeschat , waar mogelijk met behulp van wetenschappelijke modellen ;\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[9.9987292e-01 1.2702511e-04]], shape=(1, 2), dtype=float32)\n",
      "Tenslotte is er middels impact investment de mogelijkheid tot off-set negatieve risicos door positieve bijdrage .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[9.9986696e-01 1.3307898e-04]], shape=(1, 2), dtype=float32)\n",
      "De resultaten zijn afhankelijk van zowel bedrijfsspecifieke factoren als omgevingsfactoren , waardoor de uitkomsten jaar op jaar sterk uiteen kunnen lopen .\n",
      "[1]\n",
      "0.0\n",
      "tf.Tensor([[8.9951085e-05 9.9991000e-01]], shape=(1, 2), dtype=float32)\n",
      "Zolang het pensioenfonds gebruik maakt van de huidige beleggingsfondsen kan er geen beleid gevoerd worden ten aanzien van uitsluitingen en wordt ook geen actief ( stem)beleid gevoerd met betrekking tot milieu en klimaat , mensenrechten en sociale verhoudingen .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.78984654 0.21015342]], shape=(1, 2), dtype=float32)\n",
      "SPA stemde in 2016 op 1.681 aandeelhoudersvergaderingen , waarbij 22.032 agendapunten de revue passeerden .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[9.9984312e-01 1.5686924e-04]], shape=(1, 2), dtype=float32)\n",
      "Het Bestuur is verder gegaan op de eerder ingeslagen weg en heeft in 2017 de intentieverklaring I(nternationaal ) M( aatschappelijk ) V(erantwoord ) O( ndernemen ) pensioenfondsen ondertekend .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.9989011  0.00109887]], shape=(1, 2), dtype=float32)\n",
      "Naar aanleiding van de introductie van de Institutions for Occupational Retirement Provisions Directive ( IORP II ) is ook de Verklaring inzake Beleggingsbeginselen uitgebreid .\n",
      "[0]\n",
      "1.0\n",
      "tf.Tensor([[0.8938     0.10619998]], shape=(1, 2), dtype=float32)\n",
      "In addition , we have gained experience in the implementation of policy aimed at reducing the risk of heavy dependence on fossil fuels .\n",
      "[[220  12]\n",
      " [ 22 164]]\n",
      "0.9186602870813397\n",
      "0.9192040017398869\n",
      "0.9186602870813397\n",
      "0.918394824232921\n"
     ]
    }
   ],
   "source": [
    "#Test set\n",
    "X = (np.array(dft['dnb_nlp:formatted']))\n",
    "y = (np.array(dft['label']))\n",
    "X = np.array([test.encode(\"ascii\", \"ignore\").decode() for test in list(X)])\n",
    "X = np.array([test.replace('\\n',' ') for test in list(X)])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "y_predict = []\n",
    "for i in range(0,len(X_test)):\n",
    "    p,prob = predict_alt(tokenizer['nl'],model['nl'], [X_test[i]])\n",
    "    #print(\"+\" if p==1 else \"-\", end='')\n",
    "    if p != y_test[i]:\n",
    "        print(p)\n",
    "        print(y_test[i])\n",
    "        print(prob)\n",
    "        print(X_test[i])\n",
    "    y_predict = y_predict + list(p)\n",
    "\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "print(accuracy_score(y_test, y_predict))\n",
    "print(precision_score(y_test, y_predict,average='weighted'))\n",
    "print(recall_score(y_test, y_predict,average='weighted'))\n",
    "print(f1_score(y_test, y_predict,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model on complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Whole labelled set\n",
    "# df_nl['prediction'] = -1  # sentence is not processed\n",
    "\n",
    "# for row in df_nl.index:\n",
    "#     p = predict(tokenizer['nl'],model['nl'], [df_nl.loc[row, 'dnb_nlp:sentence']])\n",
    "#     print(\"+\" if p==1 else \"-\", end='')\n",
    "#     df_nl.loc[row, 'prediction'] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = dataiku.api_client()\n",
    "# project = client.get_project(\"CLIMATEANALYSIS\")\n",
    "# name = 'results_labelled_methods_incl_prediction'\n",
    "# d = dataikuapi.dss.dataset.DSSManagedDatasetCreationHelper(project,name)\n",
    "# d = d.with_store_into('filesystem_managed_solvency2')\n",
    "# d.create(name)\n",
    "# output = Dataset(name)\n",
    "# output.write_with_schema(df_nl, dropAndCreate=True)"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1661501507121,
  "creator": "A.W.M.van.Ool",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env TONE_py3_env)",
   "language": "python",
   "name": "py-dku-venv-tone_py3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "modifiedBy": "A.W.M.van.Ool",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
