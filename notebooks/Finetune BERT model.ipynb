{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import RobertaForMaskedLM, RobertaModel\n",
    "from transformers import TFRobertaForSequenceClassification, TFRobertaModel\n",
    "from transformers import pipeline\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_input(sentences,tokenizer):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "  \n",
    "    for x in sentences:\n",
    "        inputs = tokenizer.encode_plus(x, \n",
    "                                          add_special_tokens=True, \n",
    "                                          return_token_type_ids=True,\n",
    "                                          truncation=True, \n",
    "                                          max_length=max_length)\n",
    "        i, t = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "        m = [1] * len(i)\n",
    "\n",
    "        padding_length = max_length - len(i)\n",
    "\n",
    "        i = i + ([pad_token] * padding_length)\n",
    "        m = m + ([0] * padding_length)\n",
    "        t = t + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        input_ids.append(i)\n",
    "        attention_masks.append(m)\n",
    "        token_type_ids.append(t)\n",
    "  \n",
    "    return [np.asarray(input_ids), \n",
    "            np.asarray(attention_masks), \n",
    "            np.asarray(token_type_ids)]\n",
    "\n",
    "def example_to_features(input_ids, attention_masks, token_type_ids, y):\n",
    "    return {\"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"token_type_ids\": token_type_ids},y\n",
    "\n",
    "def predict(tokenizer, model, sentences):\n",
    "    tf_batch = tokenizer(sentences, max_length=256, padding=True, truncation=True, return_tensors='tf')\n",
    "    tf_outputs = model(tf_batch)\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "    label = tf.argmax(tf_predictions, axis=1)\n",
    "    label = label.numpy()\n",
    "    return label\n",
    "\n",
    "def predict_alt(tokenizer, model, sentences):\n",
    "    tf_batch = tokenizer(sentences, max_length=256, padding=True, truncation=True, return_tensors='tf')\n",
    "    tf_outputs = model(tf_batch)\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "    label = tf.argmax(tf_predictions, axis=1)\n",
    "    label = label.numpy()\n",
    "    return label, tf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = join('..','data','models','original') #folder contain RobBERT model\n",
    "MODEL_FINAL_PATH = join('..','data','models','model1') #folder contain RobBERT model#folder to save finetuned model\n",
    "OUTPUT_DATA_PATH = join('..','data','output','')\n",
    "\n",
    "model_nl = TFRobertaForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=2)\n",
    "tokenizer_nl = RobertaTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataframe with labelled sentences\n",
    "df = pd.read_excel(OUTPUT_DATA_PATH + 'df_results_mvb_big_labelled.xlsx',index_col=0)\n",
    "\n",
    "df_nl = df[df['dc:language']=='nl'].reset_index()\n",
    "dft = df_nl[df_nl.label.isna()==False]\n",
    "\n",
    "# necessary step because otherwise the tokenizer produces incorrect token during training\n",
    "for row in dft.index:\n",
    "    dft.loc[row, 'dnb_nlp:sentence'] = dft.loc[row, 'dnb_nlp:sentence'].replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (np.array(dft['dnb_nlp:formatted']))\n",
    "y = (np.array(dft['label']))\n",
    "X = np.array([test.encode(\"ascii\", \"ignore\").decode() for test in list(X)])\n",
    "X = np.array([test.replace('\\n',' ') for test in list(X)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "print(\"Train dataset shape: {0}, \\nTest dataset shape: {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "pad_token = 0\n",
    "pad_token_segment_id = 0\n",
    "max_length = 256\n",
    "\n",
    "X_test_input = convert_to_input(X_test,tokenizer_nl)\n",
    "X_train_input = convert_to_input(X_train,tokenizer_nl)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train_input[0],X_train_input[1],X_train_input[2],y_train)).map(example_to_features).shuffle(100).batch(32).repeat(5)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test_input[0],X_test_input[1],X_test_input[2],y_test)).map(example_to_features).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, \n",
    "                                     epsilon=1e-08, \n",
    "                                     clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #can be used when there are two or more label classes\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model_nl.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model_nl.fit(train_ds, epochs=5, validation_data=test_ds) #train model for finxed number of epochs\n",
    "\n",
    "model_nl.save_pretrained(save_directory=MODEL_FINAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test performance Dutch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dict()\n",
    "tokenizer = dict()\n",
    "\n",
    "model['nl'] = TFRobertaForSequenceClassification.from_pretrained(MODEL_FINAL_PATH,num_labels=2)\n",
    "tokenizer['nl'] = RobertaTokenizer.from_pretrained(MODEL_NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whole labelled set\n",
    "dft['prediction'] = -1  # sentence is not processed\n",
    "\n",
    "for row in dft.index:\n",
    "    p = predict(tokenizer['nl'],model['nl'], [dft.loc[row, 'dnb_nlp:formatted']])\n",
    "    print(\"+\" if p==1 else \"-\", end='')\n",
    "    dft.loc[row, 'prediction'] = p\n",
    "\n",
    "print(confusion_matrix(dft['label'], dft['prediction']))\n",
    "print(accuracy_score(dft['label'], dft['prediction']))\n",
    "print(precision_score(dft['label'], dft['prediction'],average='weighted'))\n",
    "print(recall_score(dft['label'], dft['prediction'],average='weighted'))\n",
    "print(f1_score(dft['label'], dft['prediction'],average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model on complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whole labelled set\n",
    "df_nl['prediction'] = -1  # sentence is not processed\n",
    "\n",
    "for row in df_nl.index:\n",
    "    p = predict(tokenizer['nl'],model['nl'], [df_nl.loc[row, 'dnb_nlp:sentence']])\n",
    "    print(\"+\" if p==1 else \"-\", end='')\n",
    "    df_nl.loc[row, 'prediction'] = p\n",
    "    \n",
    "df_nl.to_excel(OUTPUT_DATA_PATH + 'df_results_mvb_big_prediction.xlsx')"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1661501507121,
  "creator": "A.W.M.van.Ool",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "modifiedBy": "A.W.M.van.Ool",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
